{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T22:38:23.050966Z",
     "start_time": "2019-01-02T22:38:21.231640Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from functools import partial\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import pixiedust as pxdb\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict as ODict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PYTORCH\n",
    "import torch as tc\n",
    "import torchvision as tcvis\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as fu\n",
    "# TORCH HELPERS\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "import hiddenlayer as hl\n",
    "from torchsummary import summary\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# %pixie_debugger\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set(style='white', context='notebook', palette='tab10')\n",
    "%config IPCompleter.greedy = True\n",
    "%config IPCompleter.use_jedi = True\n",
    "\n",
    "\n",
    "basepath = '/home/alk/Documents/Git/Kaggles/MNIST'\n",
    "try:\n",
    "    os.chdir(basepath)\n",
    "    print(os.getcwd())\n",
    "except NotADirectoryError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T22:38:46.474465Z",
     "start_time": "2019-01-02T22:38:23.052700Z"
    }
   },
   "outputs": [],
   "source": [
    "datadir = os.getcwd() + '/data'\n",
    "filenames = ['train.csv', 'test.csv']\n",
    "datadict = ODict()\n",
    "for files in filenames:\n",
    "    try:\n",
    "        with open(datadir + '/' + files, mode='r') as csvfile:\n",
    "            datadict[files] = np.loadtxt(csvfile, delimiter=\",\", skiprows=1)\n",
    "            csvfile.close()\n",
    "        print('found file: {}'.format(files))\n",
    "    except FileNotFoundError:\n",
    "        print('skipping file ./{}'.format(files))\n",
    "\n",
    "datadict.keys(), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T22:38:46.479158Z",
     "start_time": "2019-01-02T22:38:46.475655Z"
    }
   },
   "outputs": [],
   "source": [
    "traindata = datadict[filenames[0]]\n",
    "testdata = datadict[filenames[-1]]\n",
    "\n",
    "trainlabels = traindata[:, 0].reshape(-1, 1)\n",
    "traindata = traindata[:, 1:].reshape(-1, 28, 28)\n",
    "testdata = testdata.reshape(-1, 28, 28)\n",
    "print(traindata.shape, trainlabels.shape, testdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T22:38:46.801399Z",
     "start_time": "2019-01-02T22:38:46.481594Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=True, squeeze=True)\n",
    "ax[0].imshow(traindata[-1, :, :], cmap='gray')\n",
    "ax[1].imshow(testdata[0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T23:06:43.914157Z",
     "start_time": "2019-01-02T23:06:43.692768Z"
    }
   },
   "outputs": [],
   "source": [
    "class NpDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x=traindata, y=trainlabels,\n",
    "                 transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if self.y is not None:\n",
    "            image, label = self.x[index], self.y[index]\n",
    "            label = tc.from_numpy(label).type(tc.LongTensor)\n",
    "        else:\n",
    "            image, label = self.x[index], None\n",
    "        \n",
    "        # HxWxC, UINT8\n",
    "        image = image.astype(np.uint8).reshape(28, 28, 1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# test\n",
    "test_set = NpDataset()\n",
    "print(f'target: {test_set.__getitem__(0)[1]}')\n",
    "plt.imshow(test_set.__getitem__(0)[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T23:06:44.775066Z",
     "start_time": "2019-01-02T23:06:44.157885Z"
    }
   },
   "outputs": [],
   "source": [
    "MNIST_train_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                            transforms.RandomVerticalFlip(0.25),\n",
    "                                            transforms.RandomAffine(30, (0.15,0.15),\n",
    "                                                                    (0.75,1.25), 30,\n",
    "                                                                    PIL.Image.BICUBIC,0),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.1307,),\n",
    "                                                                 (0.3081,))])\n",
    "\n",
    "MNIST_test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.1307,),\n",
    "                                                                (0.3081,))])\n",
    "\n",
    "# test\n",
    "test_set = NpDataset(transforms=MNIST_train_transform)\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, sharex=True, sharey=True, squeeze=True)\n",
    "_plots = None\n",
    "\n",
    "for axs in axes:\n",
    "    for ax in axs:\n",
    "        _plots = ax.imshow(test_set.__getitem__(np.random.randint(0, \\\n",
    "                 traindata.shape[0]))[0].numpy().reshape(28, 28),\n",
    "                 cmap='gray');\n",
    "\n",
    "_plots = plt.yticks([], [])\n",
    "_plots = plt.xticks([], [])\n",
    "_plots = plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T23:06:44.851435Z",
     "start_time": "2019-01-02T23:06:44.847470Z"
    }
   },
   "outputs": [],
   "source": [
    "NN_trainloader = DataLoader(NpDataset(transforms=MNIST_train_transform),\n",
    "                            batch_size=128,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8,\n",
    "                            pin_memory=True)\n",
    "\n",
    "NN_testloader = DataLoader(NpDataset(x=testdata, y=None,\n",
    "                                     transforms=MNIST_test_transform),\n",
    "                            batch_size=128,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T23:06:45.129211Z",
     "start_time": "2019-01-02T23:06:44.976537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dropout_fcp=0.0,\n",
    "                 wcuda=True):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        \n",
    "        self.dropout_p = dropout_fcp\n",
    "        self.wcuda = wcuda\n",
    "        \n",
    "        self.conv1a = nn.Conv2d(1, 20, 5, 1) #24\n",
    "        self.conv2a = nn.Conv2d(20, 50, 5, 1) #20\n",
    "        \n",
    "        self.conv1b = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2b = nn.Conv2d(20, 50, 5, 1)\n",
    "        \n",
    "        self.fc1a = nn.Linear(4*4*50, 500)\n",
    "        self.fc1b = nn.Linear(5*5*50, 500)\n",
    "        \n",
    "        self.fcf1 = nn.Linear(1000,100)\n",
    "        self.fcf2 = nn.Linear(100,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = fu.relu(self.conv1a(x))\n",
    "        x1 = fu.max_pool2d(x1, 2, 2)\n",
    "        x1 = fu.relu(self.conv2a(x1))\n",
    "        x1 = fu.max_pool2d(x1, 2, 2)\n",
    "        x1 = x1.view(-1, 4*4*50)\n",
    "        x1 = fu.relu(self.fc1a(x1))\n",
    "        x1 = fu.dropout(x1, p=self.dropout_p, \n",
    "                        training=self.training)\n",
    "        \n",
    "        x2 = fu.relu(self.conv1a(x))\n",
    "        x2 = fu.relu(self.conv2a(x2))\n",
    "        x2 = fu.max_pool2d(x2, 4, 4)\n",
    "        x2 = x2.view(-1, 5*5*50)\n",
    "        x2 = fu.relu(self.fc1b(x2))\n",
    "        x2 = fu.dropout(x2, p=self.dropout_p, \n",
    "                        training=self.training)\n",
    "        \n",
    "        x = tc.cat((x1, x2), 1)\n",
    "        \n",
    "        x = fu.relu(self.fcf1(x))\n",
    "        x = fu.dropout(x, p=self.dropout_p,\n",
    "                       training=self.training)\n",
    "        \n",
    "        x = self.fcf2(x)\n",
    "        x = fu.log_softmax(x, dim=1)\n",
    "        \n",
    "        if self.wcuda:\n",
    "            return x.cuda()\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "# test\n",
    "\n",
    "test_cnn = MNIST_CNN().cuda()\n",
    "print(summary(test_cnn.cuda(), (1,28,28), device='cuda'))\n",
    "hl.build_graph(model=test_cnn.cuda(), args=tc.randn(1,1,28,28).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T23:11:36.109423Z",
     "start_time": "2019-01-02T23:11:36.101501Z"
    }
   },
   "outputs": [],
   "source": [
    " def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #output = tc.argmax(output, 1).view(-1,1).type(tc.float16)\n",
    "        #print(output.shape, target.shape)\n",
    "        #print(output.type(), target.type())\n",
    "        loss = fu.nll_loss(output, target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += fu.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T23:17:53.388638Z",
     "start_time": "2019-01-02T23:17:33.976681Z"
    }
   },
   "outputs": [],
   "source": [
    "MNISTmodel = MNIST_CNN(dropout_fcp=0.25)\n",
    "optimizer = optim.Adam(MNISTmodel.parameters(), lr=0.001)\n",
    "\n",
    "device = tc.device('cuda')\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(MNISTmodel, device, NN_trainloader, optimizer, epoch, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
